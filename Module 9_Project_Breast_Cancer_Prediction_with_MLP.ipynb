{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VigneshPathivada443/FMML-COURSE-ASSIGNMENTS/blob/main/Module%209_Project_Breast_Cancer_Prediction_with_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejrV3BwMagDf"
      },
      "source": [
        "# **FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad**\n",
        "### Project for Module 9: Multi Layer Perceptron\n",
        "#### Module Coordinator: Shantanu Agrawal\n",
        "\n",
        "# Breast cancer prediction with an MLP Classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXksgX5bax-4"
      },
      "source": [
        "### Dataset Used: Breast Cancer Wisconsin (Diagnostic) Dataset\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
        "n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
        "\n",
        "This database is also available through the UW CS ftp server:\n",
        "ftp ftp.cs.wisc.edu\n",
        "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
        "\n",
        "Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "1) ID number\n",
        "2) Diagnosis (M = malignant, B = benign)\n",
        "3-32)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "a) radius (mean of distances from center to points on the perimeter)\n",
        "b) texture (standard deviation of gray-scale values)\n",
        "c) perimeter\n",
        "d) area\n",
        "e) smoothness (local variation in radius lengths)\n",
        "f) compactness (perimeter^2 / area - 1.0)\n",
        "g) concavity (severity of concave portions of the contour)\n",
        "h) concave points (number of concave portions of the contour)\n",
        "i) symmetry\n",
        "j) fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three\n",
        "largest values) of these features were computed for each image,\n",
        "resulting in 30 features. For instance, field 3 is Mean Radius, field\n",
        "13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "All feature values are recoded with four significant digits.\n",
        "\n",
        "Missing attribute values: none\n",
        "\n",
        "Class distribution: 357 benign, 212 malignant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NF-kOI_bENN",
        "outputId": "3a10a56a-2839-4414-f460-19235ed7d564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ndwj3XxQpw7rEuAYiImeGonCay7KGS9p\n",
            "To: /content/archive.zip\n",
            "100% 49.8k/49.8k [00:00<00:00, 132MB/s]\n",
            "Archive:  archive.zip\n",
            "replace data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "!pip install gdown\n",
        "!gdown 1ndwj3XxQpw7rEuAYiImeGonCay7KGS9p\n",
        "!unzip archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KrQYz5VAagDo"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "# Note that we shall be using the sklearn module for easy experimentation.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De6OPZO-agDq"
      },
      "source": [
        "### Step 1: Exploratory Data Analysis (EDA)\n",
        "We perform EDA on the data to help gain an understanding of it. This is an essential step before doing anything with that data and will help us get better results as well as interpret the results better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgBuD7m3agDr"
      },
      "outputs": [],
      "source": [
        "# Get first 5 rows of the data to see features\n",
        "breast_cancer = pd.read_csv('data.csv')\n",
        "breast_cancer.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MREhozkoagDs"
      },
      "outputs": [],
      "source": [
        "# Get counts and data types for the attributes\n",
        "breast_cancer.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEvGFth2agDt"
      },
      "outputs": [],
      "source": [
        "# Print out the shape of the dataframe\n",
        "breast_cancer.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7hlVfjAagDt"
      },
      "outputs": [],
      "source": [
        "# Print out some statistics for the data\n",
        "breast_cancer.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaiRSZMuagDv"
      },
      "outputs": [],
      "source": [
        "# Number of samples in each class\n",
        "breast_cancer.groupby('diagnosis').size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnKOsJUPagDy"
      },
      "source": [
        "### Step 2: Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "epjPxkqaagDz"
      },
      "outputs": [],
      "source": [
        "# Features \"id\" and \"Unnamed: 32\" are not useful so we remove them\n",
        "feature_names = breast_cancer.columns[2:-1]\n",
        "X = breast_cancer[feature_names]\n",
        "# \"diagnosis\" feature is our class which form the label\n",
        "y = breast_cancer.diagnosis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kda91d3vagD0"
      },
      "source": [
        "#### Transforming the prediction target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SwpC5G-3agD0"
      },
      "outputs": [],
      "source": [
        "class_le = LabelEncoder()\n",
        "# M -> 1 and B -> 0\n",
        "y = class_le.fit_transform(breast_cancer.diagnosis.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7khK7bKagD0"
      },
      "source": [
        "#### Correlation Matrix\n",
        "A matrix of correlations provides useful insight into relationships between pairs of variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJFwe0kcagD1"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(\n",
        "    data=X.corr(),\n",
        "    annot=True,\n",
        "    fmt='.2f',\n",
        "    cmap='RdYlGn'\n",
        ")\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(20, 16)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HAvY3gIagD2"
      },
      "source": [
        "### Step 3: Multi-layer Perceptron classifier evaluation after Pipeline and GridSearchCV usage\n",
        "\n",
        "We use the sklearn MLPClassifier to create our classifier and train it. In case you do not understand any of the code, I encourage you to check out the documentation first, and if you still do not understand, reach out to a TA.\n",
        "\n",
        "#### Model Parameter Tuning\n",
        "[GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) returns the set of parameters which have an imperceptible impact on model evaluation. Model parameter tuning with other steps like data preprocessing and cross-validation splitting strategy can be automated by [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class.\n",
        "\n",
        "#### Data standardization\n",
        "[Preprocessing data](http://scikit-learn.org/stable/modules/preprocessing.html) provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqiCRGJkagD3"
      },
      "source": [
        "Let's start with defining the Pipeline instance. In this case we can use different approaches like `Normalizer`, `MinMaxScaler`, `StandardScaler`, `RobustScaler`, `QuantileTransformer` for data preprocesing and `MLPClassifier` for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "blW01aVragD3"
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline(steps=[\n",
        "    ('preprocess', StandardScaler()),\n",
        "    ('classification', MLPClassifier())\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEhaMpf8agD3"
      },
      "source": [
        "Next, we need to prepare attributes with values for above steps which wanna to check by the model parameter tuning process: `activation`, `solver`, `max_iter` and `alpha`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4xyiVw0ZagD4"
      },
      "outputs": [],
      "source": [
        "random_state = 42\n",
        "mlp_activation = ['tanh', 'relu']\n",
        "mlp_solver = ['sgd', 'adam']\n",
        "mlp_max_iter = range(1000, 10000, 5000)\n",
        "mlp_alpha = [0.01, 0.1, 1]\n",
        "preprocess = [MinMaxScaler(), StandardScaler()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV-Ddn1fagD4"
      },
      "source": [
        "Next, we need to prepare supported combinations for classifier parameters including above attributes. In Multi-layer Perceptron classifier case we don't use PCA or any other feature selection techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZWBJUaG8agD4"
      },
      "outputs": [],
      "source": [
        "mlp_param_grid = [\n",
        "    {\n",
        "        'preprocess': preprocess,\n",
        "        'classification__activation': mlp_activation,\n",
        "        'classification__solver': mlp_solver,\n",
        "        'classification__random_state': [random_state],\n",
        "        'classification__max_iter': mlp_max_iter,\n",
        "        'classification__alpha': mlp_alpha\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tIW4rgOagD5"
      },
      "source": [
        "Next, we need to prepare cross-validation splitting strategy object with `StratifiedKFold` and passed it with others to `GridSearchCV`. We use the `f1 score` metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp73ZCnxagD5"
      },
      "outputs": [],
      "source": [
        "strat_k_fold = StratifiedKFold(\n",
        "    n_splits=5,\n",
        ")\n",
        "\n",
        "mlp_grid = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid=mlp_param_grid,\n",
        "    cv=strat_k_fold,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "mlp_grid.fit(X, y)\n",
        "\n",
        "# Best MLPClassifier parameters\n",
        "print(mlp_grid.best_params_)\n",
        "# Best score for MLPClassifier with best parameters\n",
        "print('\\nBest F1 score for MLP: {:.2f}%'.format(mlp_grid.best_score_ * 100))\n",
        "\n",
        "best_params = mlp_grid.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ky8WfQPagD5"
      },
      "source": [
        "#### Model evaluation\n",
        "\n",
        "Finally, we can establish the best parameters values which we pass to new feature selection and classifier instances. For example if `best_params` returned `StandardScaler` for data preprocessing and `1000`, `1`, `'tanh'` and `'adam'` values for `max_iter`, `alpha`, `activation` and `solver` classifier attributes, we use the code as below. Your result may vary so you should use whatever yielded the best parameters for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBzv-sxwagD6"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    random_state=42,\n",
        "    test_size=0.32\n",
        ")\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9j00GgkagD6"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "print('\\nData preprocessing with {scaler}\\n'.format(scaler=scaler))\n",
        "\n",
        "X_train_scaler = scaler.fit_transform(X_train)\n",
        "X_test_scaler = scaler.transform(X_test)\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "    max_iter=1000,\n",
        "    alpha=1,\n",
        "    activation='tanh',\n",
        "    solver='adam',\n",
        "    random_state=42\n",
        ")\n",
        "mlp.fit(X_train_scaler, y_train)\n",
        "\n",
        "mlp_predict = mlp.predict(X_test_scaler)\n",
        "mlp_predict_proba = mlp.predict_proba(X_test_scaler)[:, 1]\n",
        "\n",
        "print('MLP Accuracy: {:.2f}%'.format(accuracy_score(y_test, mlp_predict) * 100))\n",
        "print('MLP AUC: {:.2f}%'.format(roc_auc_score(y_test, mlp_predict_proba) * 100))\n",
        "print('MLP Classification report:\\n\\n', classification_report(y_test, mlp_predict))\n",
        "print('MLP Training set score: {:.2f}%'.format(mlp.score(X_train_scaler, y_train) * 100))\n",
        "print('MLP Testing set score: {:.2f}%'.format(mlp.score(X_test_scaler, y_test) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVR1x-pbagD6"
      },
      "source": [
        "#### Confusion Matrix\n",
        "\n",
        "Also known as an Error Matrix, is a specific table layout that allows visualization of the performance of an algorithm. The table has\n",
        "two rows and two columns that reports the number of False Positives (FP), False Negatives (FN), True Positives (TP) and True Negatives (TN). This allows more detailed analysis than accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-ZbS0YWagD7"
      },
      "outputs": [],
      "source": [
        "outcome_labels = sorted(breast_cancer.diagnosis.unique())\n",
        "\n",
        "# Confusion Matrix for MLPClassifier\n",
        "sns.heatmap(\n",
        "    confusion_matrix(y_test, mlp_predict),\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    xticklabels=outcome_labels,\n",
        "    yticklabels=outcome_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60sR64zNagD7"
      },
      "source": [
        "#### Receiver Operating Characteristic (ROC)\n",
        "\n",
        "[ROC curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD9fMLaXagD7"
      },
      "outputs": [],
      "source": [
        "# ROC for MLPClassifier\n",
        "fpr, tpr, thresholds = roc_curve(y_test, mlp_predict_proba)\n",
        "\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.title('ROC curve for MLPClassifier')\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y769AU8bagD7"
      },
      "source": [
        "#### F1-score after 5-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGQE9WomagD8"
      },
      "outputs": [],
      "source": [
        "strat_k_fold = StratifiedKFold(\n",
        "    n_splits=5,\n",
        "    )\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_std = scaler.fit_transform(X)\n",
        "\n",
        "fe_score = cross_val_score(\n",
        "    mlp,\n",
        "    X_std,\n",
        "    y,\n",
        "    cv=strat_k_fold,\n",
        "    scoring='f1'\n",
        ")\n",
        "\n",
        "print(\"MLP: F1 after 5-fold cross-validation: {:.2f}% (+/- {:.2f}%)\".format(\n",
        "    fe_score.mean() * 100,\n",
        "    fe_score.std() * 2\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NEdhxCQagD8"
      },
      "source": [
        "### Final step: Conclusions (Fill in your results)\n",
        "\n",
        "After the application of data standardization and tuning the classifier parameters we achieve the following results:\n",
        "\n",
        "*Accuracy: 98.07% (+/- 0.01%)  \n",
        "*F1-score: 97.39% (+/- 0.02%)   \n",
        "*Precision: 98.14% (+/- 0.03%)  \n",
        "*Recall: 96.70% (+/- 0.04%)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}